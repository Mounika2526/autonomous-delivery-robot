# Autonomous Delivery Robot â€“ ROS + Gazebo Simulation

A fully simulated autonomous delivery robot developed as part of **SENG 691 â€“ AI Agent Computing**.  
This project integrates **sensing, planning, and action** inside a physicsâ€‘based environment, transitioning the robot from symbolic behavior (Phaseâ€‘1) to realistic autonomous navigation driven by LiDAR and differentialâ€‘drive control (Phaseâ€‘3).

---

## ğŸš€ Project Overview

The Autonomous Delivery Robot (ADR) simulates a campus delivery agent capable of:
- Perceiving obstacles using a **Gazebo LiDAR plugin**
- Planning routes using a **gridâ€‘based A\* planner**
- Executing motion using a **differentialâ€‘drive controller**
- Adjusting its movement through **sensorâ€‘driven obstacle avoidance**
- Navigating realistically in a **Gazebo physics environment**

Phaseâ€‘3 completes the full **sense â†’ plan â†’ act â†’ evaluate** loop.

---

## ğŸ§­ Phase Progress Summary

### **Phaseâ€‘1 (Symbolic Prototype)**
- URDF model creation and RViz visualization  
- Placeholder LaserScan + hardcoded obstacles  
- Symbolic navigation (no Gazebo physics)  
- No real sensing, no real motion  

### **Phaseâ€‘2 (Simulation Foundation)**
- Robot successfully spawns in Gazebo  
- Hardware dependencies removed  
- A\* planner implemented and validated  
- Architecture restructured into perceptionâ€“planningâ€“control layers  
- Still **no real motion** and **no real LiDAR perception**

### **Phaseâ€‘3 (Full Autonomous Navigation âœ“)**
- Differentialâ€‘drive controller added â†’ robot **moves physically**  
- Gazebo LiDAR integrated â†’ **real LaserScan** data  
- Obstacleâ€‘aware navigation using sectorâ€‘based analysis  
- A\* waypoints now drive real movement  
- Completed perception â†’ planning â†’ control loop  

---

## ğŸ— System Architecture (Phaseâ€‘3)

```
User/Task â†’ Delivery Agent â†’ A* Planner â†’ Navigation Controller
                   â†“              â†‘
 Gazebo LiDAR â†’ Perception Layer â†’ Obstacle Safety Node â†’ cmd_vel â†’ Diffâ€‘Drive Plugin â†’ Gazebo World
```

### Architecture Diagram  
*(Add diagram from your PDF page 10 here)*

### Workflow Diagram  
*(Add sequence diagram from PDF page 11 here)*

---

## ğŸ” Key Phaseâ€‘3 Enhancements

### âœ… 1. Differentialâ€‘Drive Motion Integration
- Robot now moves using real `cmd_vel` commands  
- Wheel rotation, friction, collisions handled by Gazebo  
- Publishes `/odom` for waypoint tracking  
- Enables continuous forward/turn behavior  

### âœ… 2. Real LiDAR Perception
- Gazebo rayâ€‘based LiDAR publishes real `/scan`  
- Environment obstacles reflect accurately  
- Sectorâ€‘based (left/front/right) analysis enables reactive behavior  

### âœ… 3. Navigation Pipeline Completion
- A\* global planner still generates waypoints  
- Navigation controller performs:
  - Heading correction  
  - Velocity generation  
  - Obstacleâ€‘based redirection  
- Robot reaches goals meaningfully, not symbolically  

### âœ… 4. Updated Control Architecture
- Odometry â†’ Pose2D conversion for planner  
- Safety node overrides unsafe velocities  
- Modular and maintainable code structure  

---

## ğŸ“‚ Repository Structure

```
autonomous-delivery-robot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.cpp              # Navigation + differential-drive motion
â”‚   â”œâ”€â”€ delivery.cpp          # Task handling, setpoints, agent logic
â”‚   â”œâ”€â”€ obs_main.cpp          # LiDAR-based obstacle avoidance
â”‚   â”œâ”€â”€ gps.py                # Simulated GPS / pose feed
â”‚   â”œâ”€â”€ access_database.py    # (Optional) external DB integration
â”‚
â”œâ”€â”€ launch/
â”‚   â”œâ”€â”€ gazebo_nav.launch     # Full Phase-3 launch
â”‚   â”œâ”€â”€ gazebo_delivery.launch
â”‚   â”œâ”€â”€ delivery.launch
â”‚
â”œâ”€â”€ worlds/                   # Custom Gazebo world
â”œâ”€â”€ urdf/                     # Robot model w/ diff-drive + LiDAR
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ package.xml
â””â”€â”€ README.md
```

---

## ğŸ§ª Running the Project

### **1. Build**
```bash
cd ~/catkin_ws
catkin_make
source devel/setup.bash
```

### **2. Run Full Phaseâ€‘3 System**
```bash
roslaunch autonomous-delivery-robot gazebo_nav.launch
```

This loads:
- Gazebo world  
- URDF robot  
- LiDAR plugin  
- Differentialâ€‘drive controller  
- A\* planner  
- Navigation controller  
- RViz visualization  

### What You Should See
- Robot spawns in Gazebo  
- `/scan` shows real LiDAR rays  
- A\* path appears in RViz  
- Robot moves toward goal  
- Robot slows/turns when obstacles appear  
- Robot completes navigation task  

---

## ğŸ“¡ Important ROS Topics

| Topic | Description |
|-------|-------------|
| `/scan` | Real LaserScan from Gazebo LiDAR |
| `/odom` | Odometry from differential-drive |
| `/cmd_vel` | Final velocity commands |
| `/cmd_vel_nav` | Raw navigation controller output |
| `/cmd_vel_safe` | Obstacle-filtered safe commands |
| `/next_goal` | A\* waypoint publisher |

---

## ğŸ“ˆ Evidence of Improvements (Phaseâ€‘2 â†’ Phaseâ€‘3)

### Motion
- **Phaseâ€‘2:** Robot spawned but could not move  
- **Phaseâ€‘3:** Robot moves realistically using diffâ€‘drive plugin  

### Perception
- **Phaseâ€‘2:** Fake/symbolic LaserScan  
- **Phaseâ€‘3:** Real LiDAR perception from environment  

### Navigation
- **Phaseâ€‘2:** Planner disconnected from motion  
- **Phaseâ€‘3:** Robot physically follows A\* path  

### Obstacle Handling
- **Phaseâ€‘2:** None  
- **Phaseâ€‘3:** Sectorâ€‘based detection & avoidance  

### Integration
- **Phaseâ€‘2:** Modules symbolic, disconnected  
- **Phaseâ€‘3:** Full integrated sense â†’ plan â†’ act loop  

---

## âš ï¸ Known Limitations

Even with Phaseâ€‘3 improvements, some constraints remain:

- No SLAM / AMCL â†’ odometry drift possible  
- A\* global planner does not dynamically replan  
- Obstacle avoidance is reactive, not predictive  
- No multiâ€‘robot coordination  
- System is simulationâ€‘only (no real hardware yet)  

These limitations do not affect Phaseâ€‘3 correctness.

---

## ğŸ”® Future Scope

- Add SLAM/AMCL for stable localization  
- Introduce dynamic global reâ€‘planning (costmaps)  
- Expand to multiâ€‘robot coordination  
- Integrate machineâ€‘learning-based motion control  
- Deploy on real hardware with LiDAR  

---

## ğŸ¥ Phaseâ€‘3 Demonstration Summary

Your demo video should showcase:

1. Gazebo world + robot spawning  
2. RViz showing LiDAR rays, A\* path, TF frames  
3. Robot moving using diffâ€‘drive  
4. Obstacle appearing and being avoided  
5. Robot reaching destination successfully  

This validates the complete perception â†’ planning â†’ control system.

---

## ğŸ“ Resources

- GitHub Repo: https://github.com/Mounika2526/autonomous-delivery-robot  
- Demo Video Folder: (Add your Drive link)

---

## ğŸ“„ License
MIT License

---

This README reflects your **actual Phaseâ€‘3 academic achievements** and is suitable for GitHub, portfolios, and resume showcasing.
